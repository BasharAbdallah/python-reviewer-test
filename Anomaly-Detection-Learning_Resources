#!/usr/bin/python

"""
    This script will download all papers/books and rename to proper name
    if there is no copyright issue.

    TODO: download resources by item number
    TODO: add exception handler for downloader
"""
import re
import pathlib
import urllib.request

# initialize the log directory if it does not exist
pathlib.Path('resources').mkdir(parents=True, exist_ok=True)

f = open('resource_urls\\papers.txt', 'r')
for line in f:
    # print(line)
    line_splits = line.split(' | ')

    # remove all special char in file name
    file_name = re.sub(r'[\\/*?:"<>|]', "", line_splits[0])
    # strip filename length in case it is too long
    if len(file_name) > 255:
        file_name = file_name[:255]
    url = line_splits[1]

    print('Downloading', file_name, 'from', url)
    urllib.request.urlretrieve(url, "resources\\" + file_name + '.pdf')

f.close()
----------------------------------------------------------

import re
import requests


def exists(path):
    """ Utility function to check whether a web file exists
    :param path:
    :return:
    """
    r = requests.head(path)
    # print(r.status_code)
    return r.status_code == requests.codes.ok


def exists_adv(path):
    """ Utility function to check whether a web file exists
    :param path:
    :return:
    """
    # TODO: use selenium
    r = requests.head(path)
    # print(r.status_code)
    return r.status_code == requests.codes.ok


if __name__ == "__main__":
    # driver = webdriver.Firefox()
    manual_links = []
    potential_broken_links = []
    with open('README_11232019.md', encoding="utf-8") as f:
        lines = f.readlines()

    for line in lines:
        result = re.search(']\(http(.*)\)', line)
        if result:
            link = "http" + result.group(1)
            if "github" in link or "coursera" in link:
                manual_links.append(link)
            else:
                flag = exists(link)
                if not flag:
                    potential_broken_links.append(link)
                    print(link)

    print()
    for link in manual_links:
        print(link)
------------------------

#https://github.com/yzhao062/anomaly-detection-resources
